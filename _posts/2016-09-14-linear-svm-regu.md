---
layout: post
title: 多类SVM的Regularization 
description: "线性分类器笔记"
tags: [线性分类器, 斯坦福大学课程笔记]
categories: [Machine Learning, Machine Learning Basic]
---

### 回顾

对于训练集中的第i个样本$$(x_i,y_i)$$，有损失函数如下：

$$L_i= \sum_{j≠y_i}max(0,w_j^T\cdot x_i−w_{y_i}^T\cdot x_i+Δ)$$

其中，$$w_j^T\cdot x_i$$是将$$x_i$$归于j类的得分，而$$w_{y_i}^T\cdot x_i$$ 是正确归类（归于$$y_i$$类）的得分，$$\omega_i$$ 是$$W$$的第$$j$$行。

### 问题

问题一、考虑到权值向量$$\omega$$的几何意义，很容易想到，**$$\omega$$不会是唯一的**，$$\omega$$完全可以在一个小范围内摆动最后得到一样的$$L_i$$。

问题二、$$\omega$$的值如果成比例变化，那么算出来的损失也是成比例变化的，一个15的损失，只要把$$\omega$$的所有权值都乘以2，损失就变成了30。这种没有意义的等比缩放也是必须想办法抵消的。



<!-- more -->

### 正则化惩罚

解决以上两个问题的方法是在损失函数中加入惩罚项，对权值矩阵进行惩罚：
$$
R(W)=||W||^2=\sum{W\cdot W}
$$
于是加入惩罚项后的损失函数为：
$$
L=\frac{1}{N}\sum_iL_i+\lambda R(\omega)
$$
前面的项是所谓的“数据损失”（data loss）项，其现实含义是权值矩阵$$W$$作用于所有sample后产生的损失之和。

后面的项是所谓的“正则化损失”（regularization loss)，其现实含义是权值矩阵$$W$$本身的结构产生的损失：**这一项和数据无关，只跟权值矩阵有关**。

完全展开形态如下：
$$
L=\frac{1}{N}\sum_i \sum_{j≠y_i}max(0,w_j^T\cdot x_i−w_{y_i}^T\cdot x_i+Δ)+\lambda\sum{W\cdot W}
$$
重复一下符号的含义：

N代表训练集中一共有N个样本

$$i$$代表N个样本中的第$$i$$个样本

$$y_i$$:第$$i$$个样本的类别

$$j$$:所有K个类别中除了$$y_i$$以外的其他类别

$$\omega_j$$:权值向量$$W$$的第$$j$$行，$$\omega_{y_i}$$同理

$$\Delta$$:置信度

$$\lambda$$:正则化损失的系数

### 为什么正则化惩罚能解决问题？

对于问题一，因为加入了权值矩阵本身产生的损失，最终我们能得到一个唯一解

对于问题二，把权值矩阵的$$L_2$$范式作为惩罚项，**意味着我们希望权值矩阵里面的元素越小越好**，这样就防止了$$W$$毫无意义的按比例放大。

**权值矩阵的$$L_2$$范式越小，除了意味着$$W$$里面的元素数值越小外，还包含了一个信息，那就是$$W$$里面的元素分布越均匀**：向量$$\omega_1:\{1,0,0,0\}$$ 和向量$$\omega_2:\{0.25,0.25,0.25,0.25\}$$虽然数值加起来一样大，但他们的$$L_2$$范式差很远，$$\omega_1$$是1，而$$\omega_2$$是0.25。如果这两个向量都能够很好的分类，我们显然会选择$$\omega_2$$，也就是“正则化损失”更小的，因为后者分布更均匀，**而分布越均匀的权值向量意味着在分类的时候把所有的数据点都考虑进去了**。$$\omega_1$$或许分类结果也很好，但他只考虑了第一个数据点，后面三个都没考虑，为什么不考虑其他三个点也能很好的分类呢？**这意味着我们的训练集里面的样本很特殊，恰好所有样本向量里的第一个数据点都含有很多信息，足以分类（事实上这四个点的信息一样重要），如果我们遇到一个新的样本向量，他的第一个数据点并不具备足够的区分度，那我们再用$$\omega_1$$就没法分类了，这也就是所谓泛化能力差。但如果我们选择$$\omega_2$$，很可能仍然能够正常分类，因为他把所有四个点都考虑进去了。**